{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (8877, 8877) instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Apply Kalman Filter for SOG\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m vessel_data \u001b[38;5;241m=\u001b[39m \u001b[43mkalman_filter_anomaly_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvessel_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSOG\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m### DBSCAN Anomaly Detection ###\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkalman_filter_anomaly_detection\u001b[39m(df, column):\n",
      "Cell \u001b[1;32mIn[3], line 41\u001b[0m, in \u001b[0;36mkalman_filter_anomaly_detection\u001b[1;34m(df, column)\u001b[0m\n\u001b[0;32m     38\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(kf, f)\n\u001b[0;32m     40\u001b[0m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_kalman\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m state_means\n\u001b[1;32m---> 41\u001b[0m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_anomaly_kalman\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate_means\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m df[column]\u001b[38;5;241m.\u001b[39mstd()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:5815\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[0;32m   5814\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[1;32m-> 5815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\base.py:1383\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1381\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[1;32m-> 1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:5911\u001b[0m, in \u001b[0;36mSeries._construct_result\u001b[1;34m(self, result, name)\u001b[0m\n\u001b[0;32m   5908\u001b[0m \u001b[38;5;66;03m# TODO: result should always be ArrayLike, but this fails for some\u001b[39;00m\n\u001b[0;32m   5909\u001b[0m \u001b[38;5;66;03m#  JSONArray tests\u001b[39;00m\n\u001b[0;32m   5910\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 5911\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   5912\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   5914\u001b[0m \u001b[38;5;66;03m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[0;32m   5915\u001b[0m \u001b[38;5;66;03m#  would set it back to self.name\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:512\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    510\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    514\u001b[0m     manager \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\construction.py:646\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    643\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n\u001b[0;32m    644\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m maybe_infer_to_datetimelike(subarr)\n\u001b[1;32m--> 646\u001b[0m subarr \u001b[38;5;241m=\u001b[39m \u001b[43m_sanitize_ndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[0;32m    650\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mdtype, dtype)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\construction.py:705\u001b[0m, in \u001b[0;36m_sanitize_ndim\u001b[1;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allow_2d:\n\u001b[0;32m    704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData must be 1-dimensional, got ndarray of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    707\u001b[0m     )\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001b[39;00m\n\u001b[0;32m    711\u001b[0m     result \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: Data must be 1-dimensional, got ndarray of shape (8877, 8877) instead"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from pykalman import KalmanFilter\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Load AIS data with extracted features\n",
    "vessel_data = pd.read_csv('Generated_csv/vessel_data_with_features.csv')\n",
    "\n",
    "### Moving Average Anomaly Detection ###\n",
    "def moving_avg_anomaly_detection(df, column, window=5, threshold=2):\n",
    "    moving_avg_column = f'{column}_moving_avg'\n",
    "    anomaly_column = f'{column}_anomaly_moving_avg'\n",
    "    \n",
    "    # Calculate moving average\n",
    "    df[moving_avg_column] = df[column].rolling(window=window).mean()\n",
    "    \n",
    "    # Calculate anomalies based on deviations from the moving average\n",
    "    df[anomaly_column] = np.abs(df[column] - df[moving_avg_column]) > threshold * df[column].std()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply Moving Average Anomaly Detection for SOG and COG\n",
    "vessel_data = moving_avg_anomaly_detection(vessel_data, 'SOG')\n",
    "vessel_data = moving_avg_anomaly_detection(vessel_data, 'COG')\n",
    "\n",
    "### Kalman Filter Anomaly Detection ###\n",
    "def kalman_filter_anomaly_detection(df, column):\n",
    "    kf = KalmanFilter(initial_state_mean=0, n_dim_obs=1)\n",
    "    state_means, _ = kf.em(df[column].values).filter(df[column].values)\n",
    "    \n",
    "    # Save Kalman Filter parameters\n",
    "    with open(f'{column}_kalman_filter.pkl', 'wb') as f:\n",
    "        pickle.dump(kf, f)\n",
    "    \n",
    "    df[f'{column}_kalman'] = state_means\n",
    "    df[f'{column}_anomaly_kalman'] = np.abs(df[column] - state_means) > 2 * df[column].std()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply Kalman Filter for SOG\n",
    "vessel_data = kalman_filter_anomaly_detection(vessel_data, 'SOG')\n",
    "\n",
    "### DBSCAN Anomaly Detection ###\n",
    "def kalman_filter_anomaly_detection(df, column):\n",
    "    kf = KalmanFilter(initial_state_mean=df[column].iloc[0], n_dim_obs=1)\n",
    "    state_means, _ = kf.smooth(df[column].values)\n",
    "    \n",
    "    # Flatten the state_means to ensure it's 1-dimensional\n",
    "    state_means = state_means.ravel()\n",
    "    \n",
    "    # Save the Kalman filter state means\n",
    "    df[f'{column}_kalman'] = state_means\n",
    "\n",
    "    # Calculate anomalies\n",
    "    df[f'{column}_anomaly_kalman'] = np.abs(df[column] - state_means) > 2 * df[column].std()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply DBSCAN\n",
    "features = vessel_data[['SOG', 'COG', 'LAT', 'LON']].values\n",
    "vessel_data = dbscan_anomaly_detection(vessel_data, features)\n",
    "\n",
    "### Isolation Forest Anomaly Detection ###\n",
    "def isolation_forest_anomaly_detection(df, features, contamination=0.01):\n",
    "    clf = IsolationForest(contamination=contamination)\n",
    "    df['anomaly_isolation_forest'] = clf.fit_predict(features) == -1\n",
    "    \n",
    "    # Save Isolation Forest model\n",
    "    joblib.dump(clf, 'isolation_forest_model.joblib')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply Isolation Forest\n",
    "vessel_data = isolation_forest_anomaly_detection(vessel_data, features)\n",
    "\n",
    "### Autoencoder Anomaly Detection ###\n",
    "def autoencoder_anomaly_detection(df, features, encoding_dim=2, epochs=50, batch_size=32):\n",
    "    input_dim = features.shape[1]\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoder = Dense(input_dim, activation='linear')(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Normalize features\n",
    "    mean = features.mean(axis=0)\n",
    "    std = features.std(axis=0)\n",
    "    features_norm = (features - mean) / std\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(features_norm, features_norm, epochs=epochs, batch_size=batch_size, shuffle=True, validation_split=0.1)\n",
    "    \n",
    "    # Save the autoencoder model\n",
    "    autoencoder.save('autoencoder_model.h5')\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    reconstructions = autoencoder.predict(features_norm)\n",
    "    mse = np.mean(np.power(features_norm - reconstructions, 2), axis=1)\n",
    "    df['anomaly_autoencoder'] = mse > np.percentile(mse, 95)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply Autoencoder\n",
    "vessel_data = autoencoder_anomaly_detection(vessel_data, features)\n",
    "\n",
    "### Z-Score Anomaly Detection ###\n",
    "def zscore_anomaly_detection(df, column, threshold=2):\n",
    "    zscore_column = f'{column}_zscore'\n",
    "    anomaly_column = f'{column}_zscore_anomaly'\n",
    "    \n",
    "    # Calculate z-score\n",
    "    df[zscore_column] = (df[column] - df[column].mean()) / df[column].std()\n",
    "    \n",
    "    # Identify anomalies based on z-score\n",
    "    df[anomaly_column] = np.abs(df[zscore_column]) > threshold\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply Z-Score Anomaly Detection for SOG and COG\n",
    "vessel_data = zscore_anomaly_detection(vessel_data, 'SOG')\n",
    "vessel_data = zscore_anomaly_detection(vessel_data, 'COG')\n",
    "\n",
    "### Combine Anomalies ###\n",
    "def combine_anomalies(df):\n",
    "    required_columns = [\n",
    "        'SOG_zscore_anomaly', 'COG_zscore_anomaly',\n",
    "        'SOG_anomaly_moving_avg', 'COG_anomaly_moving_avg', \n",
    "        'SOG_anomaly_kalman', 'anomaly_dbscan', \n",
    "        'anomaly_isolation_forest', 'anomaly_autoencoder'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all required columns exist, if not create them with False as default\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = False\n",
    "    \n",
    "    # Combine anomalies using a voting mechanism\n",
    "    df['combined_anomaly_vote'] = (\n",
    "        df[required_columns].sum(axis=1) >= 3\n",
    "    )\n",
    "    \n",
    "    # Weighted score approach\n",
    "    weights = {\n",
    "        'SOG_zscore_anomaly': 1.0,\n",
    "        'COG_zscore_anomaly': 1.0,\n",
    "        'SOG_anomaly_moving_avg': 1.5,\n",
    "        'COG_anomaly_moving_avg': 1.5,\n",
    "        'SOG_anomaly_kalman': 2.0,\n",
    "        'anomaly_dbscan': 2.0,\n",
    "        'anomaly_isolation_forest': 2.0,\n",
    "        'anomaly_autoencoder': 2.0\n",
    "    }\n",
    "    df['combined_anomaly_weighted'] = (\n",
    "        df[list(weights.keys())] * pd.Series(weights)\n",
    "    ).sum(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Combine anomalies\n",
    "vessel_data = combine_anomalies(vessel_data)\n",
    "\n",
    "# Save the combined results to a CSV file\n",
    "vessel_data.to_csv('vessel_data_combined_anomalies1.csv', index=False)\n",
    "print(\"Combined anomaly detection results have been saved to 'vessel_data_combined_anomalies.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
